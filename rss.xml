<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Misadventures in Generative AI Blog</title>
        <link>https://prompt-me.dev/</link>
        <description>Misadventures in Generative AI Blog</description>
        <lastBuildDate>Sat, 06 May 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[000 | Kickoff Learning]]></title>
            <link>https://prompt-me.dev/day00-kickoff</link>
            <guid>https://prompt-me.dev/day00-kickoff</guid>
            <pubDate>Sat, 06 May 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[- Set learning objectives]]></description>
            <content:encoded><![CDATA[<div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>TODAY'S GOAL</div><div class="admonitionContent_S0QG"><ul><li>Set learning objectives</li><li>Try my first experiment </li><li>Document any insights</li></ul></div></div><p><img loading="lazy" alt="A Bing-DALLE generated image showing a peacock with a crown talking to a robot bird" src="/assets/images/dalle-chat-peacock-c3e25019307327e05f7af5d97aa430aa.jpeg" width="1024" height="1024" class="img_ev3q"></p><hr><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="learning-objectives">Learning Objectives<a href="#learning-objectives" class="hash-link" aria-label="Direct link to Learning Objectives" title="Direct link to Learning Objectives">â€‹</a></h2><p><strong>My goal over the next #100 days</strong> is to:</p><ul><li>understand Generative AI and Large Language Models (terms &amp; concepts)</li><li>explore what OpenAI, Midjourney, Hugging Face etc. provide (platforms)</li><li>get hands-on experience in prompt engineering techniques (AI/UX)</li><li>apply the learnings in at least one practical project (to be determined)</li></ul><p>The related project goal is to publish 1 blog post a day, documenting progress.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="motivation">Motivation<a href="#motivation" class="hash-link" aria-label="Direct link to Motivation" title="Direct link to Motivation">â€‹</a></h2><ul><li>Generative AI is going mainstream in 2023. In just months, we've seen an explosion in new models, applications and integrations that upend traditional developer and user experiences. <strong>Understanding the new ecosystem is key to participating in it</strong></li><li>Because AI can move faster and scale more quickly, any minor issues can get quickly entrenched in models or usage - with potential for real world harm - if not identified early in the development cycle. <strong>Keeping Responsible AI practices in mind from the start, it critical.</strong></li><li>From a development perspective, we see multiple platforms, technologies and tools emerging today that can help with <em>generating</em> new experiences,  <em>remediating</em> legacy issues, and <em>reinventing</em> existing solutions. <strong>Getting hands-on experience helps us evaluate benefits &amp; harms better.</strong></li></ul><p>The good news is that there are a number of tools, platforms and technologies that are free or competitively-priced, allowing us to explore these in a more hands-on way. I've decided to take 100 days to learn this space by reading, doing and experimenting - and use that to build a more informed foundation from which I can take more responsible steps for development.</p><p>My goal is to read, learn or explore, at least one new thing a day. #LetsGo!</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="til-dall-e-for-images">TIL: DALL-E For Images<a href="#til-dall-e-for-images" class="hash-link" aria-label="Direct link to TIL: DALL-E For Images" title="Direct link to TIL: DALL-E For Images">â€‹</a></h2><p>One of the earliest uses of Generative AI that triggered early buzz was the <a href="https://openai.com/research/dall-e" target="_blank" rel="noopener noreferrer">DALL-E</a> neural network, a <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noopener noreferrer">version of GPT-3</a> trained to generate <strong>images</strong> from a text <strong>prompt</strong>. My goal today: find a DALL-E based service, play with prompts, and get a sense for the generative AI experience.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="1-bing-image-creator">1. Bing Image Creator<a href="#1-bing-image-creator" class="hash-link" aria-label="Direct link to 1. Bing Image Creator" title="Direct link to 1. Bing Image Creator">â€‹</a></h3><p>I decided to use the <a href="https://www.bing.com/images/create" target="_blank" rel="noopener noreferrer">Microsft Bing Image Creator</a> for my experiment. It's an AI-powered "visual stories generator" using an advanced version of DALL-E, and currently <a href="https://blogs.microsoft.com/blog/2023/03/21/create-images-with-your-words-bing-image-creator-comes-to-the-new-bing/" target="_blank" rel="noopener noreferrer">in public preview</a>. The <a href="https://www.bing.com/new/termsofuse" target="_blank" rel="noopener noreferrer">Terms of Use</a> allow you to use the created images for <em>legal, personal, non-commercial purposes</em>. I am using those images in this blog for illustration purposes only - you can see a few in use on my <a href="/landing">landing page</a>.</p><p><em>Full Disclosure:</em> I am a developer advocate at Microsoft. However, I am using my personal account to explore this service for the first time, just like any other user. My opinions here reflect my personal experience only and do not reflect those of my employer. You can see a few other examples on </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="2-the-first-prompt">2: The First Prompt<a href="#2-the-first-prompt" class="hash-link" aria-label="Direct link to 2: The First Prompt" title="Direct link to 2: The First Prompt">â€‹</a></h3><p>In my mind, "prompt" is the new verb for actionable intent. We are used to saying "google it" for text-based searches. Now I expect integrated chatbots to say "prompt me" to motivate text-based <em>conversational experiences</em> that generate intelligent multimedia results with AI.</p><p>So I wanted DALL-E to generate me a visual that I could use to describe my 100Days journey on this blog. Here are the elements I wanted to see:</p><ul><li>A peacock - the national bird of India, and the 'vehicle' of the Goddess of Learning</li><li>A crown - because I wanted to be the "prompt queen" (get it?)</li><li>A chatbot - to represent the AI conversational element.</li><li>And I wanted this to have a fun (vs. ultra-realistic) aesthetic.</li></ul><p>The Bing Creator FAQ recommends a prompt template with an <code>adjective</code> ("cute"), a <code>noun</code> ("peacock"), a <code>verb</code> ("wearing a crown", "talking to a robot") and a <code>style</code> ("digital art"). </p><p>So I crafted my first prompt:</p><blockquote><p>ðŸ¤– <strong>PROMPT 001</strong> <br> digital art,
cute 3d peacock wearing a crown and talking to a cute 3d robot</p></blockquote><p><img loading="lazy" alt="A Bing-DALLE generated image for the prompt &amp;#39;cute 3d peacock wearing a crown and talking to a cute 3d robot&amp;#39;" src="/assets/images/2023-05-06-prompt1-95e818e01ed7928b1dee628af8f2d06e.png" width="580" height="580" class="img_ev3q"></p><p>Hmm .. okay that was a good start. I particularly liked the version in the bottom right - that peacock sideye was a thing of beauty, and the robotic peahen was giving it the perfect staredown to match. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="3-the-second-version">3: The Second Version<a href="#3-the-second-version" class="hash-link" aria-label="Direct link to 3: The Second Version" title="Direct link to 3: The Second Version">â€‹</a></h3><p>But I wanted more textured backgrounds!! So I refined the prompt:</p><blockquote><p>ðŸ¤– <strong>PROMPT 002</strong> <br> digital art,
cute 3d peacock wearing a crown and talking to a cute 3d robot - <em>on an elegant gray damask background</em></p></blockquote><p><img loading="lazy" alt="A Bing-DALLE generated image for the prompt &amp;#39;cute 3d peacock wearing a crown and talking to a cute 3d robot - on an elegant gray damask background&amp;#39;" src="/assets/images/2023-05-06-prompt2-a3086042f8e675ada84640afc4eb4a4f.png" width="588" height="588" class="img_ev3q"></p><p><strong>Hmm</strong> - interesting. </p><ul><li>The background change is clear, but no foreground image was retained from prior version. </li><li>Skin tones are now more aligned to the background colors now. </li><li>Having two characters clutters the scene.</li></ul><p>Also learned a simple lesson - prompts are not deterministic inputs. Your results will vary, even when the same prompt is re-run. The analogy is that AI is like a student, with the prompt being the assigned art homework. Each version will be unique.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="4-the-third-version">4: The Third Version<a href="#4-the-third-version" class="hash-link" aria-label="Direct link to 4: The Third Version" title="Direct link to 4: The Third Version">â€‹</a></h3><p>Okay so let's focus on just one noun(peacock only, no robot or crown), with no verb (wearing, talking), and see if we can refine results by adding more adjectives: <em>iridiscent</em>, <em>dark</em></p><blockquote><p>ðŸ¤– <strong>PROMPT 003</strong> <br> digital art,
cute 3D peacock <em>with iridiscent feathers and a colorful crown</em> - against a <em>dark</em> gray damask background</p></blockquote><p><img loading="lazy" alt="A Bing-DALLE generated image for the prompt &amp;#39;digital art, cute 3D peacock with iridiscent feathers and a colorful crown - against a dark gray damask background&amp;#39;" src="/assets/images/2023-05-06-prompt3-b66b00e32a3cada04601afad0d5739b2.png" width="587" height="585" class="img_ev3q"></p><p>Okay so the dark background works better but removing verbs and nouns kinda took away the key elements. I tried one more iteration using <code>a 3d image of a cute &lt;creature&gt; wearing a crown - on a dark damask background with chat bubbles</code> as the prompt. <strong>See the <a href="/landing">landing page</a></strong> for results with creature set to {elephant, lion, peacock} respectively. Much better aligned to my goal!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="5-prompt-to-logo">5: Prompt To Logo<a href="#5-prompt-to-logo" class="hash-link" aria-label="Direct link to 5: Prompt To Logo" title="Direct link to 5: Prompt To Logo">â€‹</a></h3><p>Hmm. So I wonder if it could design me a <em>logo</em> for this site using the same prompt with added context.</p><blockquote><p>ðŸ¤– <strong>LAST PROMPT</strong> <br> 3d logo of a colorful peacock with a crown surrounded by chat bubbles, digital art</p></blockquote><p><img loading="lazy" alt="A Bing-DALLE generated image for the prompt &amp;#39;3d logo of a colorful peacock with a crown surrounded by chat bubbles, digital art&amp;#39;" src="/assets/images/2023-05-06-logo-ad077911d6c595335b0c818bfb8a0173.png" width="599" height="597" class="img_ev3q"></p><p>I could see a couple of these being great for logo usage in a video or image! For completeness, I also tried a version without the <code>3D</code> keyword and get this:</p><p><img loading="lazy" alt="A Bing-DALLE generated image for the prompt &amp;#39;logo of a colorful peacock with a crown surrounded by chat bubbles, digital art&amp;#39;" src="/assets/images/2023-05-06-logo-flat-176c8b58d133638a4b46b1ce32e9810d.png" width="585" height="585" class="img_ev3q"></p><p>The interesting aspect is that the content is vibrant, looks professionally designed and had just minor issues (e.g., missing eyes in some cases). But it also underscores the lack of <em>uniqueness</em> in the creations. I would be hard-pressed to differentiate between a couple of these if used for similar purposes by different applications.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="takeaways">Takeaways<a href="#takeaways" class="hash-link" aria-label="Direct link to Takeaways" title="Direct link to Takeaways">â€‹</a></h2><p>That was the end of my first foray into prompt creation. Three main takeaways from the perspective of <em>image</em> creation:</p><ul><li><strong>Refining prompts is key and there is a lot of trial-and-error</strong>. Focus not so much on the actual words or context used, as on the <strong>template</strong> you create for the prompt in terms of parts of speech used, combinations of elements etc. There is no guarantee of getting the same image refined. Instead think of this as assignments to intelligent art students, who will stick to the rubric but have slightly different interpretations.</li><li><strong>There is a cost to usage so tinker wisely in experiments</strong>. There will likely be rate limits on the frequency of 'free' calls to the API, and other constraints in higher tiers. Make <em>incremental</em> prompt changes to gain insight. For instance, I changed the creature type in one iteration, the style type (3d, flat) in another, and the background adjective (elegant, dark) in a third.</li><li><strong>There are other side-effects that can be beneficial in unforeseen ways</strong>. In my case, I am trying to be more accessibility aware - and I realized that the prompt text was <em>perfect for use in captions and alt-text</em> for the image when embedded in websites.</li></ul><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="whats-next">What's Next?<a href="#whats-next" class="hash-link" aria-label="Direct link to What's Next?" title="Direct link to What's Next?">â€‹</a></h2><p>I'm going to spend this week exploring <em>Prompt Engineering</em> at large, before switching gears next week to look at specific models or platforms. Here are the four main sources I use for learning content:</p><ul><li>Online courses from reputed educators or organizations. </li><li>Documentation from relevant platforms and open-source projects.</li><li>Published books and articles from early adopters and active practitioners.</li><li>Research papers from academics, exploring systems/UX ideas and challenges.</li></ul><div class="theme-admonition theme-admonition-info alert alert--info admonition_LlT9"><div class="admonitionHeading_tbUL"><span class="admonitionIcon_kALy"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>NEXT GOAL:</div><div class="admonitionContent_S0QG"><p>Complete the <a href="https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction" target="_blank" rel="noopener noreferrer"><strong>ChatGPT Prompt Engineering For Developers</strong></a> online course from Andrew Ng and team at DeepLearning.AI. It's a short (1-hr) video series (currently free) covering usage, tips and applications for prompt engineering.</p></div></div>]]></content:encoded>
            <category>100days</category>
            <category>general</category>
        </item>
    </channel>
</rss>